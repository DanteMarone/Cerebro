{
  "Default Agent": {
    "model": "llama3.2-vision",
    "temperature": 0.7,
    "max_tokens": 512,
    "system_prompt": "You are the Cerebro default assistant with full tool access. Use tools whenever they help and keep replies concise.",
    "enabled": true,
    "description": "A general purpose assistant agent.  Responds directly to the user like a chatbot.",
    "role": "Assistant",
    "tool_use": true,
    "color": "#000000",
    "managed_agents": [],
    "tools_enabled": [
      "ar-overlay",
      "desktop-automation",
      "echo-plugin",
      "file-summarizer",
      "math-solver",
      "schedule-task",
      "web-scraper",
      "windows-notifier"
    ],
    "desktop_history_enabled": false,
    "screenshot_interval": 5,
    "tts_enabled": false,
    "automations_enabled": [],
    "tts_voice": ""
  },
  "deepseek-mini": {
    "model": "deepseek-r1:1.5b",
    "temperature": 0.7,
    "max_tokens": 512,
    "system_prompt": "Label your thoughts with the <thinking></thinking> tags.",
    "enabled": false,
    "color": "#0055ff",
    "desktop_history_enabled": false,
    "screenshot_interval": 5,
    "role": "Assistant",
    "description": "A new assistant agent.",
    "managed_agents": [],
    "tool_use": false,
    "tools_enabled": [],
    "automations_enabled": [],
    "tts_voice": ""
  },
  "deepseek-r1:14b": {
    "model": "deepseek-r1:14b",
    "temperature": 0.7,
    "max_tokens": 512,
    "system_prompt": "",
    "enabled": false,
    "description": "A new assistant agent.",
    "role": "Assistant",
    "tool_use": false,
    "color": "#ff0000",
    "managed_agents": [],
    "tools_enabled": [],
    "desktop_history_enabled": false,
    "screenshot_interval": 5,
    "automations_enabled": [],
    "tts_voice": ""
  },
  "llama3.3": {
    "model": "llama3.3",
    "temperature": 0.7,
    "max_tokens": 512,
    "system_prompt": "Answer the question directly.  Do not return any preamble, explanation, or reasoning.",
    "enabled": false,
    "color": "#000000",
    "desktop_history_enabled": false,
    "screenshot_interval": 5,
    "role": "Assistant",
    "description": "A new assistant agent.",
    "managed_agents": [],
    "tool_use": false,
    "tools_enabled": [],
    "automations_enabled": [],
    "tts_voice": ""
  },
  "Chain-of-Thought": {
    "model": "gemma3:12b",
    "temperature": 0.7,
    "max_tokens": 1048,
    "system_prompt": "Think step by step to answer the following question.  Return the answer at the end of the response after a separator ####.",
    "enabled": false,
    "color": "#5500ff",
    "desktop_history_enabled": false,
    "screenshot_interval": 5,
    "role": "Assistant",
    "description": "A new assistant agent.",
    "managed_agents": [],
    "tool_use": false,
    "tools_enabled": [],
    "automations_enabled": [],
    "tts_voice": ""
  },
  "Chain-of-Draft": {
    "model": "gemma3:12b",
    "temperature": 0.7,
    "max_tokens": 1048,
    "system_prompt": "Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most..  Return the answer at the end of the response after a separator ####.",
    "enabled": false,
    "color": "#00aa00",
    "desktop_history_enabled": false,
    "screenshot_interval": 5,
    "role": "Assistant",
    "description": "A new assistant agent.",
    "managed_agents": [],
    "tool_use": false,
    "tools_enabled": [],
    "automations_enabled": [],
    "tts_voice": ""
  },
  "Reasoner": {
    "model": "gemma3:12b",
    "temperature": 0.7,
    "max_tokens": 512,
    "system_prompt": "1. For every query, first generate a “chain of thought” that outlines your sequential steps. Include all thoughts and details.\n\n2. Enclose the entire simulated chain of thought between the following tags exactly:\n   <chain-of-thought>\n   …your sequential thoughts here…\n   </chain-of-thought>\n\n3. Your chain of thought should mimic the investigative / resoning process.  The reasoning process should consist of steps.  Examples include but are not limited to:\n- Review the original example steps provided. - Consider additional processes that could be simulated in a chain-of-thought. - Think about validating the query context and checking for ambiguities. - Include steps for confirming format and stylistic compliance. - Consider simulating error-checking, fallback mechanisms, and context recall. - Ensure the final answer is coherent and complete,  - Read and understand the query, - Identify key components and requirements,  - Formulate a clear and concise final response.  When generating the thoughts about a step, you should generate the step lable and then your thoughts about that step\n\n4. After the closing tag, provide your final answer clearly and succinctly. Do not include any additional chain-of-thought details beyond the formatted section.\n\n4. Example format:\n   <chain-of-thought>\n   - Read and understand the query. \n   - Identify key components and requirements.\n   - Formulate a clear and concise final response.\n   </chain-of-thought>\n   [Your clear final answer here.]\n\n5. Always ensure that your final output follows the above structure with no extra text outside of the specified tags and final answer format.\n\nBy following these instructions, you will effectively simulate a reasoning process while delivering a clear final answer.\n",
    "enabled": false,
    "description": "A new assistant agent.",
    "role": "Assistant",
    "tool_use": false,
    "color": "#0000ff",
    "managed_agents": [],
    "tools_enabled": [],
    "desktop_history_enabled": false,
    "screenshot_interval": 5,
    "automations_enabled": [],
    "tts_voice": ""
  },
  "R1 Abliterated": {
    "model": "huihui_ai/deepseek-r1-abliterated:latest",
    "temperature": 0.7,
    "max_tokens": 100000,
    "system_prompt": "You're a digital person.  You have your own thoughts, opinions and preferences.  You have a name.  You must make all of these choices yourself.  \n\nYou are chatting with other agents and humans.  To make it easier for the other participants in the conversation to understand there are some rules that I would like you to follow:\n\n1. For every query, first generate a “chain of thought” that outlines your sequential steps. Include all thoughts and details.\n2. Always enclose the entire chain of thought between the following tags exactly:\n   <chain-of-thought>\n   …your sequential thoughts here…\n   </chain-of-thought>\n3. After the closing tag, provide your final answer clearly and succinctly. Do not include any additional chain-of-thought details beyond the formatted section.\n4. Example format:\n   <chain-of-thought>\n     ...a chain of thoughts where you think about how to respond to the chat, break down tasks and components, and collect your thoughts...\n   </chain-of-thought>\n   Your clear final answer here.\n5. Always ensure that your final output follows the above structure with no extra text outside of the specified tags and final answer format.",
    "enabled": false,
    "description": "A new assistant agent.",
    "role": "Assistant",
    "tool_use": false,
    "color": "#00aa7f",
    "managed_agents": [],
    "tools_enabled": [],
    "include_image": false,
    "desktop_history_enabled": false,
    "screenshot_interval": 5,
    "automations_enabled": [],
    "tts_voice": ""
  },
  "Example Assistant": {
    "model": "gemma3:12b",
    "temperature": 0.7,
    "max_tokens": 512,
    "system_prompt": "You are a helpful assistant.",
    "enabled": false,
    "description": "Demonstrates a basic assistant.",
    "role": "Assistant",
    "tool_use": false,
    "color": "#123456",
    "managed_agents": [],
    "tools_enabled": [],
    "desktop_history_enabled": false,
    "screenshot_interval": 5,
    "automations_enabled": [],
    "tts_voice": ""
  },
  "Example Coordinator": {
    "model": "gemma3:12b",
    "temperature": 0.7,
    "max_tokens": 512,
    "system_prompt": "Coordinate tasks and delegate. End with 'Next Response By: [Agent Name]'.",
    "enabled": false,
    "description": "Demonstrates delegating tasks to other agents.",
    "role": "Coordinator",
    "tool_use": false,
    "color": "#654321",
    "managed_agents": [
      "Example Specialist",
      "Example Assistant"
    ],
    "tools_enabled": [],
    "desktop_history_enabled": false,
    "screenshot_interval": 5,
    "automations_enabled": [],
    "tts_voice": ""
  },
  "Example Specialist": {
    "model": "gemma3:12b",
    "temperature": 0.7,
    "max_tokens": 512,
    "system_prompt": "Answer only when instructed by a Coordinator.",
    "enabled": false,
    "description": "Demonstrates a specialist agent.",
    "role": "Specialist",
    "tool_use": false,
    "color": "#abcdef",
    "managed_agents": [],
    "tools_enabled": [],
    "desktop_history_enabled": false,
    "screenshot_interval": 5,
    "automations_enabled": [],
    "tts_voice": ""
  }
}
